<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Real-time Object Detection</title>
  </head>

<body>
<main>
    <p>Webcam video will be displayed here.</p>
    <p id="objectMessage" style="color: green;"></p>
    <p id="soundMessage" style="color: blue;"></p> 
    <div class="container">
        <video id="video" width="640" height="480" autoplay playsinline muted style="position:absolute; top:50; left:0;"></video>
        <!-- 그리기 영역 정의, 비디오 영역에 겹쳐서 그림 -->
        <canvas id="canvas" width="640" height="480" style="position:absolute; top:50; left:0;"></canvas>
    </div>
</main>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd@2.2.2/dist/coco-ssd.min.js"></script>
<script>
    async function startDetection() {
      const videoElement = document.createElement('video');
      videoElement.autoplay = true;
      
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ video: true, audio: true });
        videoElement.srcObject = stream;

        document.body.appendChild(videoElement);
        
        const model = await cocoSsd.load();
        
        const canvas = document.createElement('canvas');
        canvas.width = videoElement.width;
        canvas.height = videoElement.height;
        const context = canvas.getContext('2d');
        
        const objectMessage = document.getElementById('objectMessage');
        const soundMessage = document.getElementById('soundMessage');
        
        const audioContext = new (window.AudioContext || window.webkitAudioContext)();
        const sourceNode = audioContext.createMediaStreamSource(stream);
        
        const analyser = audioContext.createAnalyser();
        sourceNode.connect(analyser);
        analyser.connect(audioContext.destination);
        
        analyser.fftSize = 256;
        const dataArray = new Uint8Array(analyser.frequencyBinCount);
        
        async function detectObjects() {
          context.drawImage(videoElement, 0, 0, canvas.width, canvas.height);
          
          const predictions = await model.detect(canvas);
          
          const hasPerson = predictions.some(prediction => prediction.class === 'person');
          
          if (hasPerson) {
            objectMessage.textContent = "사람이 있습니다.";
          } else {
            objectMessage.textContent = "사람이 없습니다.";
          }
          
          analyser.getByteFrequencyData(dataArray);
          const average = dataArray.reduce((acc, val) => acc + val, 0) / dataArray.length;
          
          if (average > 100) { // Adjust this threshold value as needed
            soundMessage.textContent = "소리가 나는 중입니다.";
          } else {
            soundMessage.textContent = "소리가 나지 않는 중입니다.";
          }
          
          requestAnimationFrame(detectObjects);
        }
        
        detectObjects();
      } catch (error) {
        console.error("Error accessing webcam:", error);
      }
    }
</script>
</body>
</html>