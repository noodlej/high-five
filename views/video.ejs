<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Zenh87qX5JnK2Jl0vWa8Ck2rdkQ2Bzep5IDxbcnCeuOxjzrPF/et3URy9Bv1WTRi" crossorigin="anonymous">
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-OERcA2EqjJCMA+/3y+gxIOqMEjwtxJY7qPCqsdltbNJuaOe923+mo//f6V8Qbsw3" crossorigin="anonymous"></script>
  <link rel="stylesheet" href="/css/style.css"></style>
  <script src="https://code.jquery.com/jquery-3.7.0.min.js" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" crossorigin="anonymous"></script>
  <title>Video</title>
</head>
<body>
  <header>
    <%- include('header') %>
  </header>
  <main>
    <div class="container" style="position: relative; z-index: -5;">
      <button class="my-btn" style="position:absolute; top:70px; left: 300px; font-size:35px;" onclick="app()" id="cam-btn">Play</button>
      <video id="video" width="640" height="480" autoplay playsinline muted style="position:absolute; top:170px; left: 300px;;"></video>
      <canvas id="canvas" width="640" height="480" style="position:absolute; top:170px; left: 300px;"></canvas>
      <canvas id="output_canvas" width="640" height="480" style="position:absolute; top:170px; right:40;"></canvas>
      <div class="text-center" style="background-color: yellow; position:absolute; top:300px; left: 300px;">
        <h2 id="prediction"></h2>
      <div class="text" style="background-color: green; width: 500px; height: 150px;">
        <h2 id="prediction2"></h2>
      </div>
    </div>
  </main>
</body>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"> </script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/blazeface"> </script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd@2.2.2/dist/coco-ssd.min.js"></script>  

<script>
    const userInfo = JSON.parse('<%-JSON.stringify(user)%>');
    const patternLink = document.getElementById('pattern-btn');
    patternLink.href = `/user/${userInfo.id}/pattern`
    const ringLink = document.getElementById('ring-btn');
    ringLink.href = `/user/${userInfo.id}/ring`

    let animationFrame;
    let stopVideo = true
    const video = document.getElementById("video");

    const canvas = document.getElementById("canvas");
    const context = canvas.getContext("2d");
    
    const output_canvas = document.getElementById("output_canvas");
    const output_context = output_canvas.getContext("2d");
    
    const WIDTH = canvas.width;
    const HEIGHT = canvas.height;

    let boxWidth, boxHeigth, boxX, boxY, probability;
    let webcam, model;
    let openCnt = 0;
    let closeCnt = 0;

    //소리 탐지
    async function startSoundDetection() {
      const videoElement = document.createElement('video');
      //videoElement.autoplay = true;
      
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ video: true, audio: true });
        videoElement.srcObject = stream;

        const audioContext = new (window.AudioContext || window.webkitAudioContext)();
        const sourceNode = audioContext.createMediaStreamSource(stream);
        
        const analyser = audioContext.createAnalyser();
        sourceNode.connect(analyser);
        analyser.connect(audioContext.destination);
        
        analyser.fftSize = 256;
        const dataArray = new Uint8Array(analyser.frequencyBinCount);
        
        let soundThreshold = 20; // Adjust this threshold value as needed
        
        const checkSoundThreshold = () => {
          analyser.getByteFrequencyData(dataArray);
          
          const average = dataArray.reduce((acc, val) => acc + val, 0) / dataArray.length;

          if (average > soundThreshold) {
            let html = '<p>소리가 나는 중입니다. </p>';
            document.getElementById('prediction2').innerHTML = html;
          } else {
            let html = '<p>소리가 나지 않는 중입니다. </p>';
            document.getElementById('prediction2').innerHTML = html;
          }
          
          requestAnimationFrame(checkSoundThreshold);
        };
        
        checkSoundThreshold();
      } catch (error) {
        console.error("Error accessing webcam:", error);
      }
      
      document.body.appendChild(videoElement);
    }



    async function app() {
        webcam = await navigator.mediaDevices.getUserMedia({ video: true, audio: true })
        video.srcObject = webcam;
        model = await blazeface.load();
        eyeModel = await tf.loadLayersModel('https://github.com/uniqquej/tfjs/blob/main/eyes_tfjs_model_2/model.json');
        loop();
    }

    async function loop() {
        const pred = await model.estimateFaces(video, false);
        context.drawImage(video, 0, 0, 640, 480);
        if (pred.length > 0) { //얼굴 탐지되면 깜빡임 감지
            boxX = pred[0].topLeft[0];
            boxY = pred[0].topLeft[1];
            boxWidth = pred[0].bottomRight[0] - boxX;
            boxHeigth = pred[0].bottomRight[0] - boxY;
            probability = pred[0].probability[0];
            landmarks = pred[0].landmarks;
    
            context.beginPath();
            context.lineWidth = 2;
            context.strokeStyle = "#00ff00";
            context.strokeRect(boxX, boxY, boxWidth, boxHeigth);   
            context.font = "25px Arial";
            context.fillStyle = "#ffffff";
            context.fillText(`${parseFloat(probability).toFixed(2)}`, boxX, boxY);
            context.fillStyle = "#ff0000";
            context.fillRect(landmarks[0][0], landmarks[0][1], 5, 5); //오른쪽
            context.fillRect(landmarks[1][0], landmarks[1][1], 5, 5); //왼쪽
    
            let eyeWidth = landmarks[1][0] - landmarks[0][0]; // 눈 위치만 자르도록 수정하기
            console.log('eyewidth',eyeWidth);
        
            output_canvas.width = eyeWidth + 2*(landmarks[0][0]-boxX);
            output_canvas.height = boxHeigth/2;

            output_context.drawImage(video, boxX,  boxY, output_canvas.width, boxHeigth/2, 0,0, output_canvas.width, boxHeigth/2 );
                            
            let tensor = tf.browser.fromPixels(output_canvas);
            tensor = tensor.resizeNearestNeighbor([224, 224]).div(255.0);
            let x_test = tensor.expandDims(0);
            
            const eye_pred = eyeModel.predict(x_test).arraySync();
            const { values, indices } = tf.topk(eye_pred, 2);
            const index = indices.arraySync()[0];
            const probability2 = values.arraySync()[0];
            console.log(index, probability2); //index[0] = 0 > closed

            if (index[0]==1){
              //눈을 뜨면 횟수 카운트 > 한계 넘으면 알림 후 > db에 시간, 카운트 반영
              openCnt += 1;
              console.log('open',openCnt);
              if(openCnt==5){
                //알림음 울리는 코드 추가
                // 횟수 변경하기
                const res = await fetch('/', {
                  headers:{
                    "content-type":"application/json"
                  },
                  method:'POST',
                  body : JSON.stringify({
                    "user_id":userInfo.id,
                    "count": 1
                  })
                })
                if(res.status ==200){
                  openCnt=0;
                }
              }
              if(index[0]==0){
                // 눈을 뜬 후 감으면 횟수 카운트 (깜빡임 감지) > 한계 넘으면 다시 자는 것으로 간주 > 카운트 변수 초기화
                closeCnt += 1;

              }

            }

            let html = '';
              html += `${imagenetLabels[index[0]]} : ${probability2[0]}<br>`;
            document.getElementById('prediction').innerHTML = html;
        
          } else { //얼굴이 탐지되지 않았을 때, 

            let infermodel = await cocoSsd.load();
            context.drawImage(video, 0, 0);
            const predictions = await infermodel.detect(canvas);
            const person = predictions.some(prediction => prediction.class === 'person');
            console.log('Predictions: ', predictions);

            if (person) { //사람 객체가 탐지되면 뒤집힌 것으로 간주

              let html = '<p>아이가 뒤집혀 있습니다. </p>';
              document.getElementById('prediction').innerHTML = html;
              for (let i=0; i<predictions.length; i++) {
                let [x, y, width, height] = predictions[i].bbox;
                context.beginPath();
                context.lineWidth = 2;
                context.strokeStyle = "#00ff00";
                context.strokeRect(x, y, width, height);   
                context.fillStyle = "#00ff00";
                context.fillRect(x, y-20, width, 20);
                context.font = "25px Arial";
                context.fillStyle = "#ffffff";
                context.fillText(`${predictions[i].class} :  ${parseFloat(predictions[i].score).toFixed(2)}`, x, y);

              } } else { //사람 객체가 탐지되지 않으면 탐지할 객체가 없는 것으로 간주

              let html = '<p>탐지할 객체가 없습니다. </p>';
              document.getElementById('prediction').innerHTML = html;
              } 
      }





      

        animationFrame = requestAnimationFrame(loop);
    }

    app()
    startSoundDetection()
    imagenetLabels = [
        "closed",
        "open",
    ];
</script>
</html>